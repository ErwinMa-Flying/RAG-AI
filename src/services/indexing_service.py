"""
IndexingService - æ–‡ä»¶ç´¢å¼•æœåŠ¡

èŒè´£ï¼š
- ç¼–æ’å•ä¸ªæ–‡ä»¶çš„ç´¢å¼•æµç¨‹
- åè°ƒ Loaderã€Repository ç­‰ç»„ä»¶
- å¤„ç†æµç¨‹ä¸­çš„é”™è¯¯

å½“å‰é˜¶æ®µï¼ˆæç®€ç‰ˆï¼‰ï¼š
- âœ… Loader åŠ è½½æ–‡æ¡£
- âœ… æ’å…¥æ–‡ä»¶è®°å½•ï¼ˆåŒ…å« checksumï¼‰
- âœ… æ›´æ–°çŠ¶æ€ä¸º 'parsed'
- âšª æš‚ä¸åˆ†å—
- âšª æš‚ä¸å‘é‡åŒ–

åç»­æ‰©å±•ï¼š
- ğŸ”œ Splitter åˆ†å—
- ğŸ”œ Embedding å‘é‡åŒ–
- ğŸ”œ VectorStore å­˜å‚¨
"""

import logging
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Optional

from db.repositories.knowledge_file_repo import KnowledgeFileRepository
from adaptors.langchain.langchain_Loader import LangchainLoaderAdapter

logger = logging.getLogger(__name__)


class IndexingService:
    """
    æ–‡ä»¶ç´¢å¼•æœåŠ¡
    
    è´Ÿè´£å•ä¸ªæ–‡ä»¶ä»"åŸå§‹æ–‡ä»¶"åˆ°"å¯æ£€ç´¢æ•°æ®"çš„å®Œæ•´æµç¨‹
    """
    
    def __init__(
        self,
        kb_id: str,
        file_repo: Optional[KnowledgeFileRepository] = None,
        loader: Optional[LangchainLoaderAdapter] = None
    ):
        """
        åˆå§‹åŒ–ç´¢å¼•æœåŠ¡
        
        Args:
            kb_id: çŸ¥è¯†åº“ ID
            file_repo: æ–‡ä»¶è®°å½• Repositoryï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºæ–°å®ä¾‹ï¼‰
            loader: Loader Adapterï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ›å»ºæ–°å®ä¾‹ï¼‰
        """
        self.kb_id = kb_id
        self.file_repo = file_repo or KnowledgeFileRepository()
        self.loader = loader or LangchainLoaderAdapter()
        
        logger.info(f"IndexingService initialized for kb_id: {kb_id}")
    
    def index_file(self, file_path: Path) -> str:
        """
        ç´¢å¼•å•ä¸ªæ–‡ä»¶ï¼ˆå½“å‰é˜¶æ®µï¼šLoader + æ’å…¥è®°å½• + æ›´æ–°ä¸º parsedï¼‰
        
        æµç¨‹ï¼š
        1. éªŒè¯æ–‡ä»¶å­˜åœ¨
        2. è®¡ç®—æ–‡ä»¶ checksum
        3. è°ƒç”¨ Loader åŠ è½½æ–‡æ¡£ï¼ˆç”Ÿæˆå™¨ï¼‰
        4. æ’å…¥æ–‡ä»¶è®°å½•ï¼ˆknowledge_fileï¼‰
        5. æ›´æ–°çŠ¶æ€ä¸º 'parsed'
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            str: ç”Ÿæˆçš„ kf_id
        
        Raises:
            FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
            Exception: å¤„ç†å¤±è´¥
        """
        kf_id = None
        
        try:
            # ========================================
            # é˜¶æ®µ0ï¼šéªŒè¯æ–‡ä»¶
            # ========================================
            logger.info(f"Starting to index file: {file_path}")
            
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            if not file_path.is_file():
                raise ValueError(f"Path is not a file: {file_path}")
            
            # ========================================
            # é˜¶æ®µ1ï¼šè®¡ç®— checksum
            # ========================================
            logger.info(f"Calculating checksum for: {file_path.name}")
            checksum = self._calculate_checksum(file_path)
            logger.debug(f"Checksum: {checksum}")
            
            # ========================================
            # é˜¶æ®µ2ï¼šLoader åŠ è½½æ–‡æ¡£ï¼ˆç”Ÿæˆå™¨ï¼‰
            # ========================================
            logger.info(f"Loading file: {file_path.name}")
            
            # â† ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„ç­¾åï¼ˆpaths, optionsï¼‰
            raw_docs_generator = self.loader.load(
                paths=[file_path],  # â† Sequence[Path]
                options=None         # â† Optional[Dict[str, Any]]
            )
            
            # è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆå› ä¸ºåç»­éœ€è¦çŸ¥é“æ•°é‡ï¼‰
            raw_docs = list(raw_docs_generator)
            logger.info(f"Loaded {len(raw_docs)} document(s)")
            
            if not raw_docs:
                raise ValueError(f"No documents loaded from file: {file_path}")
            
            # æ‰“å°ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å‰100ä¸ªå­—ç¬¦ï¼ˆéªŒè¯åŠ è½½æˆåŠŸï¼‰
            if raw_docs:
                first_doc = raw_docs[0]
                preview = first_doc.text[:100] if hasattr(first_doc, 'text') else str(first_doc)[:100]
                logger.debug(f"First doc preview: {preview}...")
            
            # ========================================
            # é˜¶æ®µ3ï¼šæ’å…¥æ–‡ä»¶è®°å½•
            # ========================================
            logger.info(f"Inserting file record: {file_path.name}")
            file_metadata = self._get_file_metadata(file_path)
            
            kf_id = self.file_repo.insert(
                kb_id=self.kb_id,
                source_type='path',
                source_uri=str(file_path),
                file_name=file_metadata['name'],
                file_ext=file_metadata['ext'],
                file_size=file_metadata['size'],
                file_mtime=file_metadata['mtime'],
                checksum=checksum,
                parser_profile=self.loader.get_parser_name(file_path),
                version=1,
                custom_docs=0
            )
            logger.info(f"Inserted file record: {kf_id}")
            
            # ========================================
            # é˜¶æ®µ4ï¼šæ›´æ–°çŠ¶æ€ä¸º 'parsed'
            # ========================================
            self.file_repo.update_status(kf_id, 'parsed')
            logger.info(f"Updated status to 'parsed': {kf_id}")
            
            # ========================================
            # å®Œæˆ
            # ========================================
            logger.info(f"âœ… Successfully indexed file: {kf_id} ({file_path.name})")
            return kf_id
            
        except Exception as e:
            logger.error(f"âŒ Failed to index file: {file_path.name}")
            logger.error(f"Error: {e}", exc_info=True)
            
            # å¦‚æœå·²ç»æ’å…¥äº† kf_idï¼Œæ›´æ–°çŠ¶æ€ä¸º 'failed'
            if kf_id:
                try:
                    self.file_repo.update_status(kf_id, 'failed')
                    logger.info(f"Updated status to 'failed': {kf_id}")
                except Exception as update_error:
                    logger.error(f"Failed to update status to 'failed': {update_error}")
            
            raise
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„ MD5 checksum
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            str: MD5 åå…­è¿›åˆ¶å­—ç¬¦ä¸²
        """
        md5_hash = hashlib.md5()
        
        with open(file_path, 'rb') as f:
            # åˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å†…å­˜æº¢å‡º
            for chunk in iter(lambda: f.read(4096), b""):
                md5_hash.update(chunk)
        
        return md5_hash.hexdigest()
    
    def _get_file_metadata(self, file_path: Path) -> dict:
        """
        è·å–æ–‡ä»¶å…ƒä¿¡æ¯
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            dict: æ–‡ä»¶å…ƒä¿¡æ¯
                - name: æ–‡ä»¶å
                - ext: æ‰©å±•å
                - size: æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
                - mtime: ä¿®æ”¹æ—¶é—´
        """
        stat = file_path.stat()
        
        return {
            'name': file_path.name,
            'ext': file_path.suffix,
            'size': stat.st_size,
            'mtime': datetime.fromtimestamp(stat.st_mtime)
        }